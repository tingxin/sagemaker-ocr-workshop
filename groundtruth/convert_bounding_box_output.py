"""
Ground Truth Bounding Box ä»»åŠ¡è¾“å‡ºè½¬æ¢å™¨
ä¸“é—¨å¤„ç†çº¯ bounding box æ ‡æ³¨ä»»åŠ¡çš„è¾“å‡º
"""

import boto3
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple
import random


class BoundingBoxConverter:
    """Ground Truth Bounding Box è¾“å‡ºè½¬æ¢å™¨"""
    
    def __init__(self, region: str = "us-east-2"):
        self.s3 = boto3.client('s3', region_name=region)
    
    def download_output(self, bucket: str, job_name: str, local_dir: str) -> str:
        """ä¸‹è½½ Ground Truth è¾“å‡ºæ–‡ä»¶"""
        # å°è¯•ä¸åŒçš„è¾“å‡ºè·¯å¾„æ ¼å¼
        possible_prefixes = [
            f"groundtruth/output/{job_name}",
            f"{job_name}",
            f"output/{job_name}"
        ]
        
        local_dir = Path(local_dir)
        local_dir.mkdir(parents=True, exist_ok=True)
        
        manifest_path = None
        
        for prefix in possible_prefixes:
            try:
                response = self.s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
                
                for obj in response.get('Contents', []):
                    key = obj['Key']
                    if 'output.manifest' in key:
                        local_path = local_dir / 'output.manifest'
                        self.s3.download_file(bucket, key, str(local_path))
                        manifest_path = str(local_path)
                        print(f"âœ… ä¸‹è½½: {key} -> {local_path}")
                        return manifest_path
            except Exception as e:
                print(f"å°è¯•å‰ç¼€ {prefix} å¤±è´¥: {e}")
                continue
        
        print("âŒ æœªæ‰¾åˆ°è¾“å‡º manifest æ–‡ä»¶")
        return None

    def analyze_manifest_structure(self, manifest_path: str):
        """åˆ†æ manifest æ–‡ä»¶ç»“æ„"""
        print("\nğŸ” åˆ†æ manifest ç»“æ„...")
        
        with open(manifest_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if not line.strip():
                    continue
                
                item = json.loads(line)
                print(f"\nğŸ“„ è®°å½• {i+1}:")
                print(f"  Keys: {list(item.keys())}")
                
                # æŸ¥æ‰¾æ ‡æ³¨æ•°æ®
                for key, value in item.items():
                    if key != 'source-ref' and isinstance(value, dict):
                        print(f"  æ ‡æ³¨å­—æ®µ '{key}': {list(value.keys())}")
                        if 'annotations' in value:
                            annotations = value['annotations']
                            if annotations:
                                print(f"    æ ‡æ³¨æ•°é‡: {len(annotations)}")
                                print(f"    ç¬¬ä¸€ä¸ªæ ‡æ³¨: {annotations[0]}")
                
                if i >= 2:  # åªåˆ†æå‰3æ¡è®°å½•
                    break

    def convert_bounding_box_to_detection_format(self, manifest_data: List[Dict]) -> List[Dict]:
        """å°† bounding box è¾“å‡ºè½¬æ¢ä¸ºæ£€æµ‹æ ¼å¼"""
        detection_data = []
        
        for item in manifest_data:
            source_ref = item.get('source-ref', '')
            image_name = source_ref.split('/')[-1]
            
            # æŸ¥æ‰¾æ ‡æ³¨æ•°æ®å­—æ®µ
            annotation_field = None
            for key, value in item.items():
                if key != 'source-ref' and isinstance(value, dict) and 'annotations' in value:
                    annotation_field = key
                    break
            
            if not annotation_field:
                print(f"âš ï¸  {image_name}: æœªæ‰¾åˆ°æ ‡æ³¨æ•°æ®")
                continue
            
            annotation_data = item[annotation_field]
            annotations = annotation_data.get('annotations', [])
            image_size = annotation_data.get('image_size', [{}])[0]
            
            img_width = image_size.get('width', 1200)
            img_height = image_size.get('height', 800)
            
            # è½¬æ¢æ¯ä¸ªæ ‡æ³¨
            detections = []
            for ann in annotations:
                # è·å–è¾¹ç•Œæ¡†åæ ‡
                left = ann.get('left', 0)
                top = ann.get('top', 0)
                width = ann.get('width', 0)
                height = ann.get('height', 0)
                
                # è·å–ç±»åˆ«æ ‡ç­¾
                class_name = ann.get('class_id', 'unknown')
                
                # è½¬æ¢ä¸ºå››ç‚¹åæ ‡ (PaddleOCR æ ¼å¼)
                x1, y1 = int(left), int(top)
                x2, y2 = int(left + width), int(top)
                x3, y3 = int(left + width), int(top + height)
                x4, y4 = int(left), int(top + height)
                
                detections.append({
                    "transcription": class_name,  # ä½¿ç”¨ç±»åˆ«åä½œä¸º transcription
                    "points": [[x1, y1], [x2, y2], [x3, y3], [x4, y4]],
                    "class": class_name,
                    "bbox": [left, top, width, height]
                })
            
            if detections:
                detection_data.append({
                    "image": image_name,
                    "width": img_width,
                    "height": img_height,
                    "detections": detections
                })
        
        return detection_data

    def save_detection_format(self, detection_data: List[Dict], output_dir: str):
        """ä¿å­˜ä¸ºæ£€æµ‹æ ¼å¼"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # éšæœºæ‰“ä¹±æ•°æ®
        random.seed(42)
        random.shuffle(detection_data)
        
        # åˆ’åˆ†æ•°æ®é›†
        total = len(detection_data)
        train_end = int(total * 0.8)
        val_end = train_end + int(total * 0.1)
        
        splits = {
            'train': detection_data[:train_end],
            'val': detection_data[train_end:val_end],
            'test': detection_data[val_end:]
        }
        
        # ä¿å­˜æ¯ä¸ªåˆ†å‰²
        for split_name, split_data in splits.items():
            # PaddleOCR æ ¼å¼
            paddleocr_lines = []
            
            # YOLO æ ¼å¼å‡†å¤‡
            yolo_annotations = []
            
            for item in split_data:
                image_name = item['image']
                img_width = item['width']
                img_height = item['height']
                
                # PaddleOCR æ ¼å¼
                ocr_anns = []
                yolo_anns = []
                
                for det in item['detections']:
                    # PaddleOCR æ ‡æ³¨
                    ocr_anns.append({
                        "transcription": det['transcription'],
                        "points": det['points']
                    })
                    
                    # YOLO æ ¼å¼æ ‡æ³¨
                    bbox = det['bbox']
                    x_center = (bbox[0] + bbox[2]/2) / img_width
                    y_center = (bbox[1] + bbox[3]/2) / img_height
                    norm_width = bbox[2] / img_width
                    norm_height = bbox[3] / img_height
                    
                    yolo_anns.append({
                        "class": det['class'],
                        "bbox_norm": [x_center, y_center, norm_width, norm_height]
                    })
                
                if ocr_anns:
                    paddleocr_lines.append(f"{image_name}\t{json.dumps(ocr_anns, ensure_ascii=False)}")
                
                if yolo_anns:
                    yolo_annotations.append({
                        "image": image_name,
                        "annotations": yolo_anns
                    })
            
            # ä¿å­˜ PaddleOCR æ ¼å¼
            paddleocr_file = output_dir / f"label_{split_name}.txt"
            with open(paddleocr_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(paddleocr_lines))
            
            # ä¿å­˜ YOLO æ ¼å¼
            yolo_file = output_dir / f"yolo_{split_name}.json"
            with open(yolo_file, 'w', encoding='utf-8') as f:
                json.dump(yolo_annotations, f, ensure_ascii=False, indent=2)
            
            print(f"  {split_name}: {len(paddleocr_lines)} å¼ å›¾ç‰‡")
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        self.generate_class_statistics(detection_data, output_dir)
        
        return output_dir

    def generate_class_statistics(self, detection_data: List[Dict], output_dir: Path):
        """ç”Ÿæˆç±»åˆ«ç»Ÿè®¡"""
        class_counts = {}
        total_detections = 0
        
        for item in detection_data:
            for det in item['detections']:
                class_name = det['class']
                class_counts[class_name] = class_counts.get(class_name, 0) + 1
                total_detections += 1
        
        stats = {
            "total_images": len(detection_data),
            "total_detections": total_detections,
            "avg_detections_per_image": total_detections / len(detection_data),
            "class_distribution": class_counts
        }
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = output_dir / "statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
        print(f"  æ€»æ£€æµ‹æ•°: {stats['total_detections']}")
        print(f"  å¹³å‡æ¯å›¾: {stats['avg_detections_per_image']:.1f}")
        print(f"\nç±»åˆ«åˆ†å¸ƒ:")
        for class_name, count in sorted(class_counts.items(), key=lambda x: -x[1]):
            print(f"  {class_name}: {count}")


def convert_bounding_box_output(
    bucket: str,
    job_name: str,
    output_dir: str,
    region: str = "us-east-2"
):
    """è½¬æ¢ bounding box è¾“å‡º"""
    print("=" * 60)
    print("Ground Truth Bounding Box è¾“å‡ºè½¬æ¢")
    print("=" * 60)
    
    converter = BoundingBoxConverter(region)
    
    # 1. ä¸‹è½½è¾“å‡º
    print("\n[Step 1] ä¸‹è½½ Ground Truth è¾“å‡º...")
    temp_dir = Path(output_dir) / "temp"
    manifest_path = converter.download_output(bucket, job_name, str(temp_dir))
    
    if not manifest_path:
        return None
    
    # 2. åˆ†æç»“æ„
    converter.analyze_manifest_structure(manifest_path)
    
    # 3. è§£ææ•°æ®
    print("\n[Step 2] è§£ææ ‡æ³¨æ•°æ®...")
    with open(manifest_path, 'r', encoding='utf-8') as f:
        manifest_data = [json.loads(line) for line in f if line.strip()]
    
    print(f"è§£æå®Œæˆ: {len(manifest_data)} æ¡è®°å½•")
    
    # 4. è½¬æ¢æ ¼å¼
    print("\n[Step 3] è½¬æ¢ä¸ºæ£€æµ‹æ ¼å¼...")
    detection_data = converter.convert_bounding_box_to_detection_format(manifest_data)
    
    # 5. ä¿å­˜æ–‡ä»¶
    print("\n[Step 4] ä¿å­˜æ ‡ç­¾æ–‡ä»¶...")
    detection_dir = Path(output_dir) / "detection_format"
    converter.save_detection_format(detection_data, str(detection_dir))
    
    print("\n" + "=" * 60)
    print("âœ… è½¬æ¢å®Œæˆ!")
    print("=" * 60)
    print(f"\nè¾“å‡ºç›®å½•: {detection_dir}")
    print(f"  - label_*.txt: PaddleOCR æ ¼å¼")
    print(f"  - yolo_*.json: YOLO æ ¼å¼")
    print(f"  - statistics.json: æ•°æ®ç»Ÿè®¡")
    
    return detection_dir


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Ground Truth Bounding Box è¾“å‡ºè½¬æ¢')
    parser.add_argument('--bucket', required=True, help='S3 bucket åç§°')
    parser.add_argument('--job-name', required=True, help='æ ‡æ³¨å·¥ä½œåç§°')
    parser.add_argument('--output-dir', default='./bbox_output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--region', default='us-east-2', help='AWS åŒºåŸŸ')
    
    args = parser.parse_args()
    
    convert_bounding_box_output(
        bucket=args.bucket,
        job_name=args.job_name,
        output_dir=args.output_dir,
        region=args.region
    )